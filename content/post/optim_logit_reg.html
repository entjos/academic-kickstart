---
title: "Optimisation of a Logistic Regression Model using Optimx in R"
author: "Joshua Philipp Entrop"
date: '2020-05-27'
categories: [Optimisation, R]
tags: [R, logistic regression, manual optimisation]
output: html_document
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In my <a href="https://www.joshua-entrop.com/post/optim_linear_reg/">last post</a> I used the <TT>optim()</TT> command to optimise a linear regression model. In this post, I am going to take that approach a little further and optimise a logistic regression model in the same manner. Thanks to John C. Nash, I got a first glimpse into the world of optimisation functions in <TT>R</TT>. His book showed me how important it is to compare the results of different optimisation algorithms. Where I used the <TT>base</TT> optimisation function <TT>optim()</TT> in my last post, I will use <TT>optimx()</TT> from the <TT>optimx</TT> package in this post. The <TT>optimx</TT> package and function were developed by Nash and colleagues as a wrapper of the <TT>base</TT> <TT>optim()</TT> function. There are numerous advantages in using <TT>optimx()</TT> instead of <TT>optim()</TT>. In my opinion, among the most important is an easier comparison between different optimisation methods. In case someone is more interested in the variety of optimisation functions and problems that come with them, I can warmly recommend John C. Nashâ€™s book <a href="https://www.wiley.com/en-us/Nonlinear+Parameter+Optimization+Using+R+Tools-p-9781118569283"><i>Nonlinear Parameter Optimisation Using R Tools</i></a>.</p>
<p>However, coming back to my main focus: the optimisation of a logistic regression model using the <TT>optimx()</TT> function in R. For this, I would like to use the <TT>icu</TT> data set from the package <TT>aplore3</TT>. The data set contains data from 200 patients in an intensive care unit (ICU) and provides information whether the patient survived their stay or died. The particular question I would like to take a look at is whether the probability of dying during the ICU stay <span class="math inline">\(P(Y = 1)\)</span> is related to age <span class="math inline">\((x_1)\)</span> and sex <span class="math inline">\((x_2)\)</span>. In order to do so, I firstly would like to load the data set and set up our variables:</p>
<pre class="r"><code># 1. Prefix -------------------------------------------------------------------

# Remove all files from ls
rm(list = ls())

# Loading packages
require(aplore3)
require(optimx)
require(numDeriv)
require(dplyr)

# 2. Loading dataset ----------------------------------------------------------

#Reading the example data set icu from the package aplore3
icu &lt;- as.data.frame(icu)
icu$sta_n &lt;- ifelse(icu$sta == &quot;Died&quot;, 1, 0)
icu$female &lt;- ifelse(icu$gender == &quot;Female&quot;, 1, 0)</code></pre>
<p>The specific model that I would like to use is:</p>
<p><span class="math display">\[ P(Y|x_1, x_2) \sim \alpha + \beta_1  x_1 + \beta_2  x_2\]</span>
Using the logistic link-function we can find a linear function for the right side of the equation.</p>
<p><span class="math display">\[ \ln \Bigg[ \frac{P(Y|x_1, x_2)}{1 - P(Y|x_1, x_2)} \Bigg] = \alpha + \beta_1  x_1 + \beta_2  x_2\]</span></p>
<p>For this model, I would like to find the values <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> that maximize the log-likelihood function and hence, provides the best fit to our empirical data provided in the <TT>icu</TT> data set. Therefore, we also need to define the log-likelihood function for our logistic regression model. According to <a href="https://www.wiley.com/en-us/Applied+Logistic+Regression%2C+3rd+Edition-p-9780470582473">Hosmer and Lemeshow</a> the log-likelihood function for a logistic regression model can be defined as</p>
<p><span class="math display">\[ \sum_{i = 1}^{n}(y_i - \ln(\pi_i)) + (1 - y_i) * \ln(1 - \pi_i)). \]</span>
Where <span class="math inline">\(\pi\)</span> is defined using the sigmoid function as</p>
<p><span class="math display">\[ P(Y|x_1, x_2) = \pi = \frac{\exp(\alpha + \beta_1  x_1 + \beta_2  x_2)}{1 + \exp(\alpha + \beta_1  x_1 + \beta_2  x_2)}. \]</span></p>
<p>For the optimisation in R we need to define the log-likelihood function as a function in <TT>R</TT>. Additionally, we need to add the constrain <span class="math inline">\(0 &lt; \pi &lt; 1\)</span> to our like-likelihood function, since we are interested in a probability <span class="math inline">\(\pi\)</span> which needs to be in the range between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. We can use an <TT>if</TT> statement in <TT>R</TT> to include our constrain to our R function. For all parameter values that return a value of <span class="math inline">\(\pi\)</span> that is out of the bounds, we set the value to a very high number, for instance <span class="math inline">\(10^{200}\)</span>. Using these high numbers for values outside the bounds, the optimisation algorithm will dismiss these parameter values from our solutions. Note that we calculate <TT>-sum()</TT>, since we want to find the maximum of the log-likelihood function.</p>
<pre class="r"><code># 3. Define log-likelihood function for logistic regression model -------------
# (see applied logistic regression)
negll &lt;- function(par){
  
  #Extract guesses for alpha and beta1
  alpha &lt;- par[1]
  beta1 &lt;- par[2]
  beta2 &lt;- par[3]
  
  #Define dependent and independent variables
  y  &lt;- icu$sta_n
  x1 &lt;- icu$age
  x2 &lt;- icu$female
  
  #Calculate pi and xb
  xb &lt;- alpha + beta1 * x1 + beta2 * x2
  pi &lt;- exp(xb) / (1 + exp(xb))
  
  #Set high values for 0 &lt; pi &lt; 1
  if(any(pi &gt; 1) | any(pi &lt; 0)) {
    val &lt;- 1e+200
  } else {
    val &lt;- -sum(y * log(pi) + (1 - y) * log(1 - pi))
  }
  val
}</code></pre>
<p>Additionally to our log-likelihood function, it is also useful to specify the gradient function for our log-likelihood. This is not necessary for all optimisation algorithms, however, it improves the testing for convergence. Hence, we can obtain better estimates by also supplying the gradient function. According to Hosmer and Lemeshow, the gradient function of the log-likelihood function is defined as</p>
<p><span class="math display">\[
g_\alpha(\pi) = \sum(y_i - \pi_i)
\]</span>
<span class="math display">\[
g_{x_j}(\pi) = \sum(x_{ji} * (y_i - \pi_i)).
\]</span></p>
<p>In our case we yield 3 gradient functions</p>
<p><span class="math display">\[
g_\alpha(\pi) = \sum(y_i - \pi_i)
\]</span>
<span class="math display">\[
g_{x_1}(\pi) = \sum(x_{1_i} * (y_i - \pi_i))
\]</span></p>
<p><span class="math display">\[
g_{x_2}(\pi) = \sum(x_{2_i} * (y_i - \pi_i)).
\]</span></p>
<p>We can then use these 3 functions to calculate the gradients in R. Also here we need to use <TT>-sum()</TT> for the gradient functions.</p>
<pre class="r"><code># 4. Define fradient function for logistic regression model -------------------
# (see applied logistic regression)
negll.grad &lt;- function(par){
  
  #Extract guesses for alpha and beta1
  alpha &lt;- par[1]
  beta1 &lt;- par[2]
  beta2 &lt;- par[3]
  
  #Define dependent and independent variables
  y  &lt;- icu$sta_n
  x1 &lt;- icu$age
  x2 &lt;- icu$female
  
  #Create output vector
  n &lt;- length(par[1])
  gg &lt;- as.vector(rep(0, n))
  
  #Calculate pi and xb
  xb &lt;- alpha + beta1 * x1 + beta2 * x2
  pi &lt;- exp(xb) / (1 + exp(xb))
  
  #Calculate gradients for alpha and beta1
  gg[1] &lt;- -sum(y - pi)
  gg[2] &lt;- -sum(x1 * (y - pi))
  gg[3] &lt;- -sum(x2 * (y - pi))
  
  return(gg)
}</code></pre>
<p><TT>R</TT> also provides functions to estimate a numerical approximation of the gradient function. One of these function is <TT>grad()</TT> from the <TT>numDeriv</TT> package. It is useful to double check your analytic gradient function using one of these numerical approximations. Since, <TT>optimx()</TT> uses the <TT>grad()</TT> function for doing this, we are also going to use this function</p>
<pre class="r"><code># 4.1 Compare gradient function with numeric approximation of gradient ========
# compare gradient at 0, 0, 0
mygrad &lt;- negll.grad(c(0, 0, 0))
numgrad &lt;- grad(x = c(0, 0, 0), func = negll)

all.equal(mygrad, numgrad)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>We see, that the results from our analytic gradient function are identical to the results using the <TT>grad()</TT> function. So we can proceed and use the <TT>optimx()</TT> function to find the maximum of our log-likelihood function. As a first guess we use <span class="math inline">\(0\)</span> as initial value for all unknown parameters.</p>
<pre class="r"><code># 4. Find maximum of log-likelihood function ----------------------------------
opt &lt;- optimx(par = c(alpha  = 0,
                      beta_1 = 0, 
                      beta_2 = 0), 
              fn = negll, 
              gr = negll.grad, 
              control = list(trace = 0, 
                             all.methods = TRUE))

# print reulsts of optimisation
# remove not needed information for purpose of presentation 
summary(opt, order = &quot;convcode&quot;) %&gt;% 
  select(-value, -niter, -gevals, -fevals)</code></pre>
<pre><code>##                 alpha     beta_1      beta_2 convcode  kkt1 kkt2 xtime
## Nelder-Mead -3.055741 0.02756878 -0.01126589        0 FALSE TRUE  0.00
## L-BFGS-B    -3.056762 0.02758493 -0.01130317        0  TRUE TRUE  0.00
## nlm         -3.056691 0.02758409 -0.01131100        0  TRUE TRUE  0.00
## nlminb      -3.056690 0.02758409 -0.01131159        0  TRUE TRUE  0.00
## Rcgmin      -3.056691 0.02758409 -0.01131098        1  TRUE TRUE  0.04
## Rvmmin       0.000000 0.00000000  0.00000000       21 FALSE TRUE  0.00
## BFGS               NA         NA          NA     9999    NA   NA  0.00
## CG                 NA         NA          NA     9999    NA   NA  0.00
## spg                NA         NA          NA     9999    NA   NA  0.01
## ucminf             NA         NA          NA     9999    NA   NA  0.00
## newuoa             NA         NA          NA     9999    NA   NA  0.01
## bobyqa             NA         NA          NA     9999    NA   NA  0.00
## nmkb               NA         NA          NA     9999    NA   NA  0.00
## hjkb               NA         NA          NA     9999    NA   NA  0.01</code></pre>
<p>A value of <span class="math inline">\(0\)</span> in the <TT>convcode</TT> column of the output indicates, that the algorithm converged. Even though multiple algorithms converged and gave us a value for our three unknown parameters, they all provide slightly different estimates. Therefore, I think it would be interesting to compare our estimates with the estimates from the commonly used <TT>glm()</TT> function. Below I wrote a small function that estimates the mean differences in the estimates from the different optimisation methods and the <TT>glm</TT> model.</p>
<pre class="r"><code># 5. Estimate regression coeficents using glm ---------------------------------
glm_model &lt;- glm(sta_n ~ age + female, 
                 data = icu,
                 family = binomial(link = &quot;logit&quot;))

# Print coefficents
coef(glm_model)</code></pre>
<pre><code>## (Intercept)         age      female 
## -3.05669068  0.02758409 -0.01131098</code></pre>
<pre class="r"><code># 6. Comparing results from optimx and glm ------------------------------------
glm_results &lt;- unname(coef(glm_model))
coef_opt &lt;- coef(opt)

lapply(1:nrow(coef_opt), function(i){
    
    optimisation_algorithm &lt;- attributes(coef_opt)$dimnames[[1]][i]

    mle_glm1 &lt;- (coef_opt[i, &quot;alpha&quot; ] - glm_results[1])
    mle_glm2 &lt;- (coef_opt[i, &quot;beta_1&quot;] - glm_results[2])
    mle_glm3 &lt;- (coef_opt[i, &quot;beta_2&quot;] - glm_results[3])
    
    mean_difference &lt;- mean(mle_glm1, mle_glm2, mle_glm3, na.rm = TRUE)
    
    data.frame(optimisation_algorithm, mean_difference)
    
  }) %&gt;% 
    bind_rows() %&gt;% 
  filter(!is.na(mean_difference)) %&gt;% 
  mutate(mean_difference = abs(mean_difference)) %&gt;% 
  arrange(mean_difference)</code></pre>
<pre><code>##   optimisation_algorithm mean_difference
## 1                 Rcgmin    1.887799e-08
## 2                    nlm    5.856574e-08
## 3                 nlminb    2.071622e-07
## 4               L-BFGS-B    7.134885e-05
## 5            Nelder-Mead    9.493966e-04
## 6                 Rvmmin    3.056691e+00</code></pre>
<p>This shows that the <TT>Rcgmin</TT> algorithm yield the most similar results to the estimates from the <TT>glm</TT> model. However, most of the algorithms in the table provide estimates similar to the estimates from the <TT>glm</TT> model, which indicates that our optimisation of the logistic regression model using the log-likelihood function and the gradient function worked out well.</p>
