---
title: "Optimisation of an exponential survival model using Optimx in R"
author: "Joshua Philipp Entrop"
date: '2020-06-29'
output: html_document
categories: [Optimisation, R]
tags: [R, survival analysis, manual optimisation]
draft: TRUE
---



<p>In this blog post, we will fit a Poisson regression model by minimising its likelihood function using <TT>optimx()</TT> in <TT>R</TT>. As example we will use the lung cancer data set included in the <TT>{survival}</TT> package. The data set includes information on 228 lung cancer patients from the North Central Cancer Treatment Group (NCCTG). Specifically, we will estimate the survival of lung cancer patients by sex and age using a simple Poisson regression model. The general survival function <span class="math inline">\(S(t)\)</span> for our model can be specified as</p>
<p><span class="math display">\[ S(t) = \exp(-\lambda t) \]</span></p>
<p>where the hazard function <span class="math inline">\(h(t)\)</span> is equal to</p>
<p><span class="math display">\[ h(t) = \lambda = \exp(\alpha + \beta_1  x_{female} + \beta_2  x_{age}). \]</span>
To get started we first need to load all the packages that we will need for our estimations and set up the data set.</p>
<pre class="r"><code># 1. Prefix -------------------------------------------------------------------

# Remove all files from ls
rm(list = ls())

# Loading packages
require(survival)
require(flexsurv)
require(optimx)
require(numDeriv)
require(dplyr)
require(tibble)
require(car)

# 2. Loading dataset ----------------------------------------------------------

#Reading the example data set lung from the survival package
lung &lt;- as.data.frame(survival::lung)

#Recode dichotomous vairables
lung$female &lt;- ifelse(lung$sex == 2, 1, 0)
lung$status_n &lt;- ifelse(lung$status == 2, 1, 0)</code></pre>
<p>At this point we would usually call <TT>survreg()</TT> or <TT>flexsurvreg()</TT> to fit our Possion model. However, in this post we will use the likelihood function of our Possion regression model together with <TT>optimx()</TT> from the <TT>{optimx}</TT> package instead. For this we first need to find the likelihood function for our model and then use <TT>optimx()</TT> to find the values for our parameters, that maximise our likelihood function.</p>
<p>The log likelihood (<span class="math inline">\(\ln L_i\)</span>) for a survival model can be specified as</p>
<p><span class="math display">\[ \ln L_i = d_i \ln h(t_i) + \ln S(t_i). \]</span>
Where <span class="math inline">\(d_i\)</span> indicates whether the <span class="math inline">\(i^{th}\)</span> subject experienced an event (1) or did not experienced an event (0) during follow up and <span class="math inline">\(t_i\)</span> is the time from the beginning of follow up until censoring.</p>
<p>To obtain the log likelihood function for our Possion model we can substitute <span class="math inline">\(h(t)\)</span> and <span class="math inline">\(S(t)\)</span> with our previously defined hazard and survival function respectively. Thus, we get the following equation for our log likelihood</p>
<p><span class="math display">\[\ln L_i = d_i \ln \lambda - \lambda t_i \]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is defined as mentioned above.</p>
<p>The next step now is to write our likelihood function as a function in R, which can be maximised by <TT>optimx()</TT>. Please keep in mind, that <TT>optimx()</TT> by default minimises the function we pass to it. However, in our case we need to find the maximum of our likelihood function. To yield the estimates, that maximise our function we can just ask <TT>optimx()</TT> to minimise the negative of our likelihood. For more information on the setting up the likelihood function for <TT>optimx()</TT> or <TT>optim()</TT> please take a look at <a href="https://www.joshua-entrop.com/post/optim_linear_reg/">this</a> earlier blog post.</p>
<p>Lets set up our likelihood function in R.</p>
<pre class="r"><code># 3. Define log-likelihood function for Poisson regression model --------------
negll &lt;- function(par){
  
  #Extract guesses for alpha, beta1 and beta2
  alpha &lt;- par[1]
  beta1 &lt;- par[2]
  beta2 &lt;- par[3]
  
  #Define dependent and independent variables
  t  &lt;- lung$time
  d  &lt;- lung$status_n
  x1 &lt;- lung$female
  x2 &lt;- lung$age
  
  #Calculate lambda
  lambda &lt;- exp(alpha + beta1 * x1 + beta2 * x2)
  
  #Estimate negetive log likelihood value
  val &lt;- -sum(d * log(lambda) - lambda * t)
  
  return(val)
}</code></pre>
<p>To improve the optimisation we can further pass the gradient function of our likelihood function to our <TT>optimx()</TT> call. After partially deriving <span class="math inline">\(L_i\)</span> for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta_i\)</span> we yield the two following equations for the gradient of <span class="math inline">\(L_i\)</span>.</p>
<p><span class="math display">\[ \sum d_i - \lambda_i t_i  = 0\]</span></p>
<p><span class="math display">\[ \sum d_i x_{ij} - \lambda_i x_{ij} t = 0\]</span></p>
<p>Given these gradient equations we can now define our gradient function in R. For this we need to create a function, that returns the gradient for each of our unknown parameters. Since we have three unknown parameters our gradient function will return a vector <TT>gg</TT> with three values.</p>
<pre class="r"><code># 4. Define fradient function for logistic regression model -------------------
# (see applied logistic regression)
negll.grad &lt;- function(par){
  
  #Extract guesses for alpha and beta1
  alpha &lt;- par[1]
  beta1 &lt;- par[2]
  beta2 &lt;- par[3]
  
  #Define dependent and independent variables
  t  &lt;- lung$time
  d  &lt;- lung$status_n
  x1 &lt;- lung$female
  x2 &lt;- lung$age
  
  #Create output vector
  n &lt;- length(par[1])
  gg &lt;- as.vector(rep(0, n))
  
  #Calculate pi and xb
  lambda &lt;- exp(alpha + beta1 * x1 + beta2 * x2)
  
  #Calculate gradients for alpha and beta1
  gg[1] &lt;- -sum(d - lambda * t)
  gg[2] &lt;- -sum(d * x1 - lambda * x1 * t)
  gg[3] &lt;- -sum(d * x2 - lambda * x2 * t)
  
  return(gg)
}</code></pre>
<p>We can compare the results of our gradient function with the results from the <TT>grad()</TT> function included in the <TT>{numDeriv}</TT> package, before we begin with the optimisation of our functions. This is just a check to be sure our gradient function works properly.</p>
<pre class="r"><code># 4.1 Compare gradient function with numeric approximation of gradient ========
# compare gradient at 0, 0, 0
mygrad &lt;- negll.grad(c(0, 0, 0))
numgrad &lt;- grad(x = c(0, 0, 0), func = negll)

all.equal(mygrad, numgrad)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Looks like our gradient functions does a good job. Now that we have all the functions and information we need for our optimisation, we can call <TT>optimx()</TT> and pass our functions to it.</p>
<p>The output of <TT>optimx()</TT> provides us with estimates for our coefficients and information regarding whether the optimisation algorithm converged (<TT>convcode == 0</TT>) besides the maximum value of the negative log likelihood obtained by the different algorithms. Hence, it is useful to sort the results by <TT>convcode</TT> and <TT>value</TT>.</p>
<pre class="r"><code># 4. Find minimum of log-likelihood function ----------------------------------

# Passing names to the values in the par vector improves readability of results
opt &lt;- optimx(par = c(alpha = 0, beta_female = 0, beta_age = 0), 
              fn = negll,
              gr = negll.grad,
              hessian = TRUE,
              control = list(trace = 0, all.methods = TRUE))

# Show results for optimisation alogrithms, that convergered (convcode == 0)
summary(opt, order = &quot;value&quot;) %&gt;%
  rownames_to_column(&quot;algorithm&quot;) %&gt;% 
  filter(convcode == 0) %&gt;% 
  select(algorithm, alpha, beta_female, beta_age, value)</code></pre>
<pre><code>##     algorithm     alpha beta_female   beta_age    value
## 1      nlminb -6.840606  -0.4809343 0.01561870 1156.099
## 2        BFGS -6.840627  -0.4809436 0.01561907 1156.099
## 3    L-BFGS-B -6.840636  -0.4809316 0.01561902 1156.099
## 4 Nelder-Mead -6.832428  -0.4814582 0.01547911 1156.099</code></pre>
<p>The summary of our <TT>optimx()</TT> call shows, that the <TT>nlminb</TT> algorithm yielded the best result. Lets see if this result is equal to the results we will get, if we use <TT>flexsurvreg</TT> from the <TT>{flexsurv}</TT> package to fit our desired model.</p>
<pre class="r"><code># 5. Estimate regression coeficents using glm ---------------------------------
pois_model &lt;- flexsurvreg(Surv(time, status_n == 1) ~ female + age, 
                          data = lung,
                          dist = &quot;exp&quot;)

# 6. Comparing results from optimx and glm ------------------------------------
pois_results &lt;- unname(coef(pois_model))
coef_opt &lt;- coef(opt)

lapply(1:nrow(coef_opt), function(i){
  
  opt_name &lt;- attributes(coef_opt)$dimnames[[1]][i]
  
  mle_pois1 &lt;- (coef_opt[i, 1] - pois_results[1])
  mle_pois2 &lt;- (coef_opt[i, 2] - pois_results[2])
  mle_pois3 &lt;- (coef_opt[i, 3] - pois_results[3])
  
  mean_dif &lt;- mean(mle_pois1, mle_pois2, mle_pois3, na.rm = TRUE)
  
  data.frame(opt_name, mean_dif)
  
}) %&gt;% 
  bind_rows() %&gt;% 
  filter(!is.na(mean_dif)) %&gt;% 
  mutate(mean_dif = abs(mean_dif)) %&gt;% 
  arrange(mean_dif)</code></pre>
<pre><code>##      opt_name     mean_dif
## 1      nlminb 2.678650e-07
## 2        BFGS 2.091779e-05
## 3    L-BFGS-B 2.911668e-05
## 4 Nelder-Mead 8.178948e-03
## 5          CG 6.816256e+00
## 6      Rvmmin 6.840606e+00</code></pre>
<p>The mean difference between our estimates and the estimates obtained by using <TT>flexsurvreg()</TT> are close to zero. Seems like our optimisation using the log likelihood did a good job.</p>
<p>However, the result obtained with <TT>flexsurvreg()</TT> provided us with estimates for the standard errors (SEs) of our hazard estimates, too. Since the measurement of uncertainty is at the heart of statistics, I think it is worthwhile to obtain the SEs for our estimates with the information provided by our <TT>optimx()</TT> call. For a more detailed discussion on how this is done please take a look at one of my previous blog posts <a href="https://www.joshua-entrop.com/post/optim_logit_reg_se/">here</a>.</p>
<p>Letâ€™s obtain the SEs for our model by using the results from our <TT>optimx()</TT> call and compare them with the SEs obtained by <TT>flexsurvreg()</TT>.</p>
<pre class="r"><code># 7. Estimate the standard error ----------------------------------------------

#Extract hessian matrix for Rcgmin optimisation
hessian_m &lt;- attributes(opt)$details[&quot;nlminb&quot;, &quot;nhatend&quot;][[1]]

# Estimate se based on hession matrix
fisher_info &lt;- solve(hessian_m)
prop_se  &lt;- sqrt(diag(fisher_info))

# Compare the estimated se from our model with the one from the flexsurv model
# Note use res.t to get the estimates on the reale scale without transformaiton
ses &lt;- data.frame(se_nlminb = prop_se, 
                  se_felxsurvreg = pois_model$res.t[, &quot;se&quot;]) %&gt;%
  print()</code></pre>
<pre><code>##          se_nlminb se_felxsurvreg
## rate   0.587477415    0.587136471
## female 0.167094278    0.167094285
## age    0.009105681    0.009097022</code></pre>
<pre class="r"><code>all.equal(ses[,&quot;se_nlminb&quot;], ses[, &quot;se_felxsurvreg&quot;])</code></pre>
<pre><code>## [1] &quot;Mean relative difference: 0.000457798&quot;</code></pre>
<p>Looks like we got nearly equal results. Let us now use these information an estimate the 95% confidence intervals (CIs) for our estimates.</p>
<pre class="r"><code># 8. Estimate 95%CIs using estimation of SE -----------------------------------

# Extracting estimates from Rcgmin optimisaiton
coef_test &lt;- coef(opt)[&quot;nlminb&quot;,]

# Compute 95%CIs
upper &lt;- coef_test + 1.96 * prop_se
lower &lt;- coef_test - 1.96 * prop_se

# Print 95%CIs
data.frame(Estimate = coef_test, 
           CI_lower = lower, 
           CI_upper = upper, 
           se       = prop_se)</code></pre>
<pre><code>##               Estimate     CI_lower    CI_upper          se
## alpha       -6.8406062 -7.992061931 -5.68915046 0.587477415
## beta_female -0.4809343 -0.808439062 -0.15342949 0.167094278
## beta_age     0.0156187 -0.002228433  0.03346584 0.009105681</code></pre>
<p>One usual way to plot the results of our estimation is plotting the survival function <span class="math inline">\(S(t)\)</span>. Since, uncertainty is important I also want to plot the CI for our survival function. To obtain estimates for the SE of the survival function <span class="math inline">\(S(t)\)</span> is a little bit more complicated. However, the amazing <TT>deltaMethod()</TT> function included in the <TT>{car}</TT> package makes it fairly easy to obtain estimates for the SEs. We just need to provide <TT>deltaMethod()</TT> with a vector of our coefficients, our covariance matrix and the computation for which we would like to obtain the SEs.</p>
<pre class="r"><code># 9. Plot survival curve with 95%-CI ------------------------------------------

# 9.1 Use Delta Method to compute CIs across time of follow-up ================

# Get coefficents for nlminb optimisation
nlminb_coef &lt;- coef(opt)[&quot;nlminb&quot;, ]

# Compute CIs for a 60 year of female across time
surv_optim_female &lt;- lapply(as.list(seq(0.01, 1000.01, 10)), function(t){
  
  g &lt;- paste(&quot;exp(-exp(alpha + beta_female + 60 * beta_age) *&quot;, t, &quot;)&quot;)
  
  fit &lt;- deltaMethod(nlminb_coef, g, solve(hessian_m))
  
  data.frame(time     = t,
             estimate = fit[, &quot;Estimate&quot;],
             ci_low   = fit[, &quot;2.5 %&quot;],
             ci_up    = fit[, &quot;97.5 %&quot;])
  
}) %&gt;% 
  bind_rows()</code></pre>
<p>We can now use these information to plot our survival curve <span class="math inline">\(S(t)\)</span> together with a grey shaded area that indicates the CIs for our survival function.</p>
<pre class="r"><code># 9.2 Plot survival curve with CIs ============================================
plot(surv_optim_female$time,
     surv_optim_female$estimate,
     ylim = c(0, 1),
     type = &quot;n&quot;,
     xlab = &quot;Time in Days&quot;,
     ylab = &quot;S(t)&quot;,
     main = &quot;Survival after lung cancer \n for 60 year old females&quot;)
polygon(c(surv_optim_female$time, rev(surv_optim_female$time)),
        c(surv_optim_female$ci_low, rev(surv_optim_female$ci_up)),
        border = NA,
        col = &quot;grey&quot;)
lines(surv_optim_female$time,
     surv_optim_female$estimate)
legend(0, 0.15,
       fill = &quot;grey&quot;,
       &quot;95% CI&quot;)</code></pre>
<p><img src="/post/optim_pois_reg_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>To sum it up, in this blog post we learned how to fit a Possion regression model using the log likelihood function in R instead of going the usual way of calling <TT>survreg()</TT> or <TT>flexsurvreg()</TT>. I think doing this is a good way of gaining an deeper understanding of how estimates for regression models are obtained. In my next post I will take this a step further and show how we can fit a Weibull regression model in R using the log likelihood function in combination with <TT>optimx()</TT>.</p>
