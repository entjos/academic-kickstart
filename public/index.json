[{"authors":["admin"],"categories":null,"content":"Joshua Entrop is a student with passion for health data and statistical analyses. My research interests are not limited to certain diseases or health issues. Instead, I am interested in statistical and methodological challenges that come along with a variety of research topics. Within the research process, the phase of data analysis is the most interesting one for me. I love to apply different statistical models to data, to reveal new insights, which cannot be seen at rst glance.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/joshua-philipp-entrop/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/joshua-philipp-entrop/","section":"authors","summary":"Joshua Entrop is a student with passion for health data and statistical analyses. My research interests are not limited to certain diseases or health issues. Instead, I am interested in statistical and methodological challenges that come along with a variety of research topics.","tags":null,"title":"Joshua Philipp Entrop","type":"authors"},{"authors":null,"categories":null,"content":"\rIn this post I would like to show how to manually optimise a linear regression model using the optim command in R. Usually if you learn how to fit a linear regression model in R, you would learn how to use the lm command to do this. However, if you would like to know how to do this manually, examples are rare.\nIf you want to optimise a function, the most important question of course is which function should be optimised? As it is taught in most statistic classes this function is in case of a linear regression model the distribution of the residuals (R). We know that this distribution follows a normal distriibution with mean 0 and a unknown standart deviation \\(\\sigma\\):\n\\[\\sum_{i = 1}^{i = n} R_i \\sim N(0, \\sigma)\\]\nWhere R equals:\n\\[R_i = y_1 - \\hat{y_i}\\]\nThus, we are interested in a function for \\(\\hat{y_i}\\) which minimises the residuals and hence, gives us the best estimates for the function of interest. Taking all this information together we can write the function we would like to optimise, shown below.\n#Reading the example data set icu from the package aplore3\rlibrary(aplore3)\ry = icu$sys #Set our depended variable\rx1 = icu$age #Set our fist independed variable\rx2 = as.numeric(icu$gender) #Set our second independed variable\r#Define our liklihood function we like to optimise\rll_lm \u0026lt;- function(par, y, x1, x2){\ralpha \u0026lt;- par[1]\rbeta1 \u0026lt;- par[2]\rbeta2 \u0026lt;- par[3]\rsigma \u0026lt;- par[4]\rR = y - alpha - beta1 * x1 - beta2 * x2\r-sum(dnorm(R, mean = 0, sigma, log = TRUE))\r}\rA function that can be used for the  optim  command needs to have a  par  argument, which includes the unknown parameters. The  par  arguments needs a vector with initial values or guesses for all unkown parameters. As shown in the example above the  par  argument includes initial values for all 4 unknown parameters. In this example the first value of the  par  arguments equals alpha, the second beta1, the third beta2 and the fourth sigma, which is our unknown standart deviation of the normal distribution of our residuals. Additionally, we included our two independent variables x1 and x2 and our dependened variable y as function arguments in the optim call.\nAlso note, that I used the  log  argument of the  dnorm  function to get the logaritmic values of the dnorm function. This is necessary, if we wold like to sum the singel liklihood values instead of taking the product of them.\nThe linear model we would like to fit in this example is:\n\\[E(Y|X) = \\alpha + \\beta_1 x_1 + \\beta_2 x_2\\]\nHence, the residuals for this model can be calculated as:\n\\[R = y - \\alpha - \\beta_1 x_1 - \\beta_2 x_2\\]\nSince we know that these residuals follow a normal distribution with mean 0, we just need to find the standardt deviation for the normal distribution of the residuals and the values for our coefficents, that fits best our data. To do this, we would like to minimise the sum of errors. This is done by the  optim  command. However, since the  optim  command always maximises functions, we just need to put a minus before our summation.\nBefore, we run the  optim  command we also need to find good guesses for our estimates, since the initial parameter values which are chosen for the optimisation influences our estimates. In this case we just calculate the conditional means for our subgroups and use them as guess for our coefficients.\nest_alpha \u0026lt;- mean(icu$sys)\rest_beta1 \u0026lt;- mean(icu$sys[icu$age \u0026gt;= 40 \u0026amp; icu$age \u0026lt;= 41]) - mean(icu$sys[icu$age \u0026gt;= 41 \u0026amp; icu$age \u0026lt;= 52])\rest_beta2 \u0026lt;- mean(icu$sys[icu$gender == \u0026quot;Male\u0026quot;]) - mean(icu$sys[icu$gender == \u0026quot;Female\u0026quot;])\rest_sigma \u0026lt;- sd(icu$sys)\rNow we can use the optim function to search for our maximmum liklihood estimates (mles) for the different coefficents. For the optim function, we need the function we like to optimise, in this case  ll_lm , our guesses for the estimates and the empirical data we want to use for the optimisation.\nmle_par \u0026lt;- optim(fn = ll_lm, #Function to be optimised\rpar = c(alpha = est_alpha, #Initial values\rbeta1 = est_beta1, beta2 = est_beta2, sigma = est_sigma), y = icu$sys, #Empirical data from the data set icu\rx1 = icu$age,\rx2 = as.numeric(icu$gender))\rmle_par$par #Showing estimates for unknown parameters\r## alpha beta1 beta2 sigma ## 123.99854056 0.06173058 3.37040256 32.77094809\rIf we now compare our estimates with the results of the  lm  command for the same model, we can see slight differences in the esitmates, which may be due to different optimisation alogrithms or due to our initial guesses for the parameters.\nsummary(lm(sys ~ age + as.numeric(gender), data = icu))\r## ## Call:\r## lm(formula = sys ~ age + as.numeric(gender), data = icu)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -98.544 -20.510 -1.445 18.884 122.272 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 124.39209 9.32708 13.337 \u0026lt;2e-16 ***\r## age 0.06276 0.11738 0.535 0.593 ## as.numeric(gender) 3.09867 4.83773 0.641 0.523 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 33.05 on 197 degrees of freedom\r## Multiple R-squared: 0.003889, Adjusted R-squared: -0.006224 ## F-statistic: 0.3845 on 2 and 197 DF, p-value: 0.6813\rThis approach for a manuell optimisation of a linear regression model can also be adopted for other linear regression model with more coefficents. In this case the formula for the residuals needs to be adjusted to the structure of the model that you like to fit.\n","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587859200,"objectID":"d8a8dea17c101475a650073609718055","permalink":"/post/optim_linear_reg/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/optim_linear_reg/","section":"post","summary":"In this post I would like to show how to manually optimise a linear regression model using the optim command in R. Usually if you learn how to fit a linear regression model in R, you would learn how to use the lm command to do this.","tags":null,"title":"Optimisation of a Linear Regression Model","type":"post"}]